{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysWWuNnLgi-2"
      },
      "source": [
        "# Transform Photos to Monet Paintings with CycleGANs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBBIbbQSgm-I"
      },
      "source": [
        "This project implements a CycleGAN (Generative Adversarial Network) to transfer the artistic style of Claude Monet's paintings to regular photographs.\n",
        "It demonstrates unpaired image-to-image translation, where the model learns to capture the essence of Monet's style—color palettes, brush strokes, and impressionistic feel—and apply it to new photos without requiring directly corresponding pairs of photos and paintings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GIR6lyYg_R_"
      },
      "source": [
        "### Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pvX8bZlgLlS"
      },
      "outputs": [],
      "source": [
        "!pip -q install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar544wSVgTuh",
        "outputId": "f45c60b6-adf9-4a1e-ee20-fc6a1033cf4c"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import plot_model\n",
        "from keras.layers import TFSMLayer\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQkS0rzmg8Km"
      },
      "source": [
        "The following code initializes a TPU strategy if a TPU is available. Otherwise, it defaults to other available strategies (GPU/CPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TClZSqAPhXiw",
        "outputId": "4e3d69de-951f-4847-b84d-a5498c29ceba"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    print('Running on TPU:', tpu.master())\n",
        "except ValueError:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
        "    print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDWvOk-0hRqS"
      },
      "source": [
        "### Defining Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sjieZSdhfAJ"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "IMAGE_SIZE = [256, 256]\n",
        "\n",
        "def decode_image(file_path):\n",
        "    \"\"\"\n",
        "    Reads and decodes a JPEG image file to a TensorFlow tensor,\n",
        "    normalizes it to [-1, 1], and reshapes it.\n",
        "    \"\"\"\n",
        "    image = tf.io.read_file(file_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
        "    image = tf.image.resize(image, IMAGE_SIZE) # Ensure resizing here\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_dataset(directory):\n",
        "    \"\"\"\n",
        "    Creates a tf.data.Dataset from JPEG images in a directory.\n",
        "    \"\"\"\n",
        "    dataset = tf.data.Dataset.list_files(str(Path(directory) / \"*.jpg\"))\n",
        "    dataset = dataset.map(decode_image, num_parallel_calls=AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xyIyhZBsH79"
      },
      "source": [
        "## Understanding Image Style Transfer\n",
        "\n",
        "Image Style Transfer is a computer vision task that involves recomposing an image in the style of another. Essentially, it combines the content of one image with the artistic style (textures, colors, patterns) of a reference style image. CycleGANs allow this to be done even when there are no direct pairs of content and styled images for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spcaEXI0sTfY"
      },
      "source": [
        "<center><img src=\"https://junyanz.github.io/CycleGAN/images/teaser.jpg\" width=\"100%\" style=\"vertical-align:middle;margin:20px 0px\"></center>\n",
        "<p style=\"color:gray; text-align:center;\"><i>Examples of image-to-image translations by CycleGAN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qol9FmW7soZd"
      },
      "source": [
        "## CycleGANs Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUukWF3Xs-Ev"
      },
      "source": [
        "### Recap on Vanilla GANs\n",
        "\n",
        "A standard Generative Adversarial Network (GAN) consists of two neural networks: a Generator ($\\boldsymbol G$) and a Discriminator ($\\boldsymbol D$).\n",
        "*   The **Generator** tries to create realistic data (e.g., images) from random noise or a source domain image.\n",
        "*   The **Discriminator** tries to distinguish between real data and fake data generated by $\\boldsymbol G$.\n",
        "They are trained in an adversarial process: $\\boldsymbol G$ aims to fool $\\boldsymbol D$, while $\\boldsymbol D$ aims to get better at identifying fakes. This minimax game leads to $\\boldsymbol G$ producing increasingly realistic data\n",
        "\n",
        "### What's Novel About CycleGANs?\n",
        "\n",
        "CycleGANs introduce two key innovations for unpaired image-to-image translation:\n",
        "\n",
        "1.  **Unpaired Training Data:** Unlike many style transfer methods that require paired examples (e.g., an exact photo and its stylized version), CycleGANs can learn from two separate, unpaired collections of images (e.g., a set of landscape photos and a set of Monet paintings).\n",
        "\n",
        "2.  **Cycle Consistency Loss:** This is the core idea. If you translate an image from domain X to domain Y, and then translate it back from Y to X, you should ideally get the original image back. This enforces a structural and content consistency during translation.\n",
        "    *   Let $G: X \\rightarrow Y$ be the generator from domain X to Y, and $F: Y \\rightarrow X$ be the generator from Y to X.\n",
        "    *   Cycle consistency means $F(G(x)) \\approx x$ for $x \\in X$, and $G(F(y)) \\approx y$ for $y \\in Y$.\n",
        "    *   The loss incurred from these cycles (e.g., L1 distance between $x$ and $F(G(x))$) is added to the standard adversarial losses, guiding the generators to learn meaningful mappings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EouvoVeGhjm3"
      },
      "source": [
        "## Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqCeIpLVhlwi"
      },
      "source": [
        "The dataset used is a subset of the [\"I'm Something of a Painter Myself\" Kaggle competition](https://www.kaggle.com/competitions/gan-getting-started/data), containing Monet paintings and landscape photographs. We have download a prepared subset of 300 images for each domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KDnROUdhofH",
        "outputId": "3d21d86c-a6aa-4c4f-cdad-e2ab5de2f049"
      },
      "outputs": [],
      "source": [
        "DATA_ROOT_DIR = \"data\"\n",
        "MONET_DIR = Path(DATA_ROOT_DIR) / \"monet_jpg_300\"\n",
        "PHOTO_DIR = Path(DATA_ROOT_DIR) / \"photo_jpg_300\"\n",
        "\n",
        "MONET_FILENAMES = tf.io.gfile.glob(str(MONET_DIR / '*.jpg'))\n",
        "print('Monet JPG Files:', len(MONET_FILENAMES))\n",
        "\n",
        "PHOTO_FILENAMES = tf.io.gfile.glob(str(PHOTO_DIR / '*.jpg'))\n",
        "print('Photo JPG Files:', len(PHOTO_FILENAMES))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRnthxKKhs_X"
      },
      "source": [
        "Create `tf.data.Dataset` objects for efficient data loading and batching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99MNcsiEhyZK",
        "outputId": "f1843558-e9dc-4d6c-ea94-4b53d5141ea7"
      },
      "outputs": [],
      "source": [
        "monet_ds = load_dataset(str(MONET_DIR)).batch(1)\n",
        "photo_ds = load_dataset(str(PHOTO_DIR)).batch(1)\n",
        "\n",
        "print(\"Monet Dataset:\", monet_ds)\n",
        "print(\"Photo Dataset:\", photo_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLj6r4m7jOgh"
      },
      "source": [
        "Let's visualize a sample from each dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "aE8QJ0AdjSs0",
        "outputId": "b903b15c-d933-4c73-c2d5-281620ce1a25"
      },
      "outputs": [],
      "source": [
        "example_monet = next(iter(monet_ds))\n",
        "example_photo = next(iter(photo_ds))\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(121)\n",
        "plt.title('Sample Photo')\n",
        "plt.imshow(example_photo[0] * 0.5 + 0.5) # Denormalize for viewing\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Sample Monet Painting')\n",
        "plt.imshow(example_monet[0] * 0.5 + 0.5) # Denormalize for viewing\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UENVIcN8l1Xt"
      },
      "source": [
        "## Building the Generator\n",
        "The Generator architecture is typically an encoder-decoder structure with skip connections (like U-Net). It uses downsampling blocks to encode the input image into a compact representation, and upsampling blocks to decode this representation into the target style image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkpoUwbMmAW-"
      },
      "source": [
        "### Defining the Downsampling Block\n",
        "Each downsampling block consists of a Convolutional layer, Instance Normalization (implemented via Group Normalization), and a LeakyReLU activation. Instance Normalization is preferred over Batch Normalization in style transfer tasks as it normalizes features per-sample, preserving style information better. We use `tf.keras.layers.GroupNormalization` with `groups` equal to the number of `filters` to achieve Instance Normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpYEi4rvmCcN"
      },
      "outputs": [],
      "source": [
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "def downsample(filters, size, apply_instancenorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    result = keras.Sequential()\n",
        "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    if apply_instancenorm:\n",
        "        # Use GroupNormalization as InstanceNormalization by setting groups=filters\n",
        "        result.add(layers.GroupNormalization(groups=filters, gamma_initializer=gamma_init))\n",
        "\n",
        "    result.add(layers.LeakyReLU())\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4MI9mJjmSR-"
      },
      "source": [
        "### Defining the Upsampling Block\n",
        "Each upsampling block uses a Transposed Convolutional layer (Conv2DTranspose) to increase spatial dimensions, followed by Instance Normalization (via Group Normalization) and ReLU activation. Dropout can be applied to some upsampling layers to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebXyb95ZmUqz"
      },
      "outputs": [],
      "source": [
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    result = keras.Sequential()\n",
        "    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                      padding='same',\n",
        "                                      kernel_initializer=initializer,\n",
        "                                      use_bias=False))\n",
        "\n",
        "    # Use GroupNormalization as InstanceNormalization by setting groups=filters\n",
        "    result.add(layers.GroupNormalization(groups=filters, gamma_initializer=gamma_init))\n",
        "\n",
        "    if apply_dropout:\n",
        "        result.add(layers.Dropout(0.5))\n",
        "\n",
        "    result.add(layers.ReLU())\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciwmv7zzmbhu"
      },
      "source": [
        "### Assembling the Generator\n",
        "The full generator model connects downsampling blocks, a series of residual blocks (not explicitly defined as separate blocks here but implied by the U-Net like structure with skip connections), and upsampling blocks. Skip connections concatenate feature maps from downsampling layers to corresponding upsampling layers, helping preserve low-level details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woy0r7w1meIo"
      },
      "outputs": [],
      "source": [
        "def Generator():\n",
        "    inputs = layers.Input(shape=[256,256,3])\n",
        "\n",
        "    # Downsampling stack\n",
        "    down_stack = [\n",
        "        downsample(64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)\n",
        "        downsample(128, 4), # (bs, 64, 64, 128)\n",
        "        downsample(256, 4), # (bs, 32, 32, 256)\n",
        "        downsample(512, 4), # (bs, 16, 16, 512)\n",
        "        downsample(512, 4), # (bs, 8, 8, 512)\n",
        "        downsample(512, 4), # (bs, 4, 4, 512)\n",
        "        downsample(512, 4), # (bs, 2, 2, 512)\n",
        "        downsample(512, 4), # (bs, 1, 1, 512)\n",
        "    ]\n",
        "\n",
        "    # Upsampling stack\n",
        "    up_stack = [\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
        "        upsample(512, 4), # (bs, 16, 16, 1024)\n",
        "        upsample(256, 4), # (bs, 32, 32, 512)\n",
        "        upsample(128, 4), # (bs, 64, 64, 256)\n",
        "        upsample(64, 4), # (bs, 128, 128, 128)\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
        "                                  strides=2,\n",
        "                                  padding='same',\n",
        "                                  kernel_initializer=initializer,\n",
        "                                  activation='tanh') # Output normalized to [-1, 1]\n",
        "\n",
        "    x = inputs\n",
        "\n",
        "    # Downsampling through the model\n",
        "    skips = []\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    skips = reversed(skips[:-1]) # Exclude the last one (bottleneck)\n",
        "\n",
        "    # Upsampling and establishing the skip connections\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = layers.Concatenate()([x, skip])\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk28UD74mjKq"
      },
      "source": [
        "Visualize the Generator architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Is8BtpaSmmVS",
        "outputId": "66586f07-15f9-438a-8cfc-fb4db1b15b55"
      },
      "outputs": [],
      "source": [
        "gen_model_example = Generator()\n",
        "plot_model(gen_model_example, show_shapes=True, show_layer_names=True, dpi=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FrrECaoSO8ta",
        "outputId": "a80c356b-9f85-4b66-afae-6dacfc25557c"
      },
      "outputs": [],
      "source": [
        "gen_model_example.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPBiW2dGm0fu"
      },
      "source": [
        "## Building the Discriminator\n",
        "The Discriminator is a convolutional neural network that classifies input images (or patches of images, in a PatchGAN) as real or fake. It typically consists of several downsampling convolutional layers. Instance Normalization (via Group Normalization) is also used here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhrUAauum_pn"
      },
      "outputs": [],
      "source": [
        "def Discriminator():\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n",
        "\n",
        "    x = inp\n",
        "\n",
        "    down1 = downsample(64, 4, apply_instancenorm=False)(x) # (bs, 128, 128, 64) - No InstanceNorm on first layer typically\n",
        "    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
        "    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
        "\n",
        "    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
        "    conv = layers.Conv2D(512, 4, strides=1, # Strides=1 for PatchGAN-like behavior\n",
        "                         kernel_initializer=initializer,\n",
        "                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
        "\n",
        "    # Use GroupNormalization as InstanceNormalization. Input to this layer has 512 channels from 'conv'.\n",
        "    norm1 = layers.GroupNormalization(groups=512, gamma_initializer=gamma_init)(conv)\n",
        "    leaky_relu = layers.LeakyReLU()(norm1)\n",
        "\n",
        "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
        "    last = layers.Conv2D(1, 4, strides=1, # Output is a single channel (logit for real/fake)\n",
        "                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "\n",
        "    return keras.Model(inputs=inp, outputs=last)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3CiUVZtnDwu"
      },
      "source": [
        "Visualize the Discriminator architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        },
        "id": "lpo41RWGnH-_",
        "outputId": "159535a1-0b59-4cfc-e502-7ed4e68c7dba"
      },
      "outputs": [],
      "source": [
        "disc_model_example = Discriminator()\n",
        "plot_model(disc_model_example, show_shapes=True, show_layer_names=True, dpi=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "PWRZtSEVOzcu",
        "outputId": "17775d6c-7039-42d0-c368-de620d6a8930"
      },
      "outputs": [],
      "source": [
        "#Parameters\n",
        "disc_model_example.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtheyufpnR1O"
      },
      "source": [
        "## Building the CycleGAN Model\n",
        "\n",
        "A CycleGAN requires two Generators ($G_{AB}$: Photo $\\rightarrow$ Monet, $G_{BA}$: Monet $\\rightarrow$ Photo) and two Discriminators ($D_A$: distinguishes real Photos from fake Photos generated by $G_{BA}$; $D_B$: distinguishes real Monet paintings from fake Monet paintings generated by $G_{AB}$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G39wjS1TnVn5"
      },
      "outputs": [],
      "source": [
        "with strategy.scope(): # For distributed training (TPU/multi-GPU)\n",
        "    monet_generator = Generator() # Transforms photos to Monet-esque paintings (G_AB)\n",
        "    photo_generator = Generator() # Transforms Monet paintings to photos (G_BA)\n",
        "\n",
        "    monet_discriminator = Discriminator() # Differentiates real Monet and fake Monet (D_B)\n",
        "    photo_discriminator = Discriminator() # Differentiates real photos and fake photos (D_A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9suWBZjnaQM"
      },
      "source": [
        "The `CycleGan` class encapsulates the entire model, its components, and the custom training step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAfGRHCOnddN"
      },
      "outputs": [],
      "source": [
        "class CycleGan(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        monet_generator, # G_AB (photo -> monet)\n",
        "        photo_generator, # G_BA (monet -> photo)\n",
        "        monet_discriminator, # D_B (discriminates monet)\n",
        "        photo_discriminator, # D_A (discriminates photo)\n",
        "        lambda_cycle=10, # Weight for cycle consistency loss\n",
        "        lambda_identity=0.5 # Weight for identity loss (relative to lambda_cycle)\n",
        "    ):\n",
        "        super(CycleGan, self).__init__()\n",
        "        self.m_gen = monet_generator\n",
        "        self.p_gen = photo_generator\n",
        "        self.m_disc = monet_discriminator\n",
        "        self.p_disc = photo_discriminator\n",
        "        self.lambda_cycle = lambda_cycle\n",
        "        self.lambda_identity_factor = lambda_identity\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        m_gen_optimizer,\n",
        "        p_gen_optimizer,\n",
        "        m_disc_optimizer,\n",
        "        p_disc_optimizer,\n",
        "        gen_loss_fn,\n",
        "        disc_loss_fn,\n",
        "        cycle_loss_fn,\n",
        "        identity_loss_fn\n",
        "    ):\n",
        "        super(CycleGan, self).compile()\n",
        "        self.m_gen_optimizer = m_gen_optimizer\n",
        "        self.p_gen_optimizer = p_gen_optimizer\n",
        "        self.m_disc_optimizer = m_disc_optimizer\n",
        "        self.p_disc_optimizer = p_disc_optimizer\n",
        "        self.gen_loss_fn = gen_loss_fn\n",
        "        self.disc_loss_fn = disc_loss_fn\n",
        "        self.cycle_loss_fn = cycle_loss_fn\n",
        "        self.identity_loss_fn = identity_loss_fn\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, batch_data):\n",
        "        real_monet, real_photo = batch_data # Monet is domain B, Photo is domain A\n",
        "\n",
        "        # Calculate actual identity loss weight\n",
        "        current_lambda_identity = self.lambda_identity_factor * self.lambda_cycle\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # Forward cycle: Photo (A) -> Monet (B) -> Photo (A')\n",
        "            fake_monet = self.m_gen(real_photo, training=True) # G_AB(A) = B_fake\n",
        "            cycled_photo = self.p_gen(fake_monet, training=True) # G_BA(B_fake) = A_cycled\n",
        "\n",
        "            # Backward cycle: Monet (B) -> Photo (A) -> Monet (B')\n",
        "            fake_photo = self.p_gen(real_monet, training=True) # G_BA(B) = A_fake\n",
        "            cycled_monet = self.m_gen(fake_photo, training=True) # G_AB(A_fake) = B_cycled\n",
        "\n",
        "            # Identity mapping: Generator should not change images from its target domain\n",
        "            same_monet = self.m_gen(real_monet, training=True) # G_AB(B) ideally should be B\n",
        "            same_photo = self.p_gen(real_photo, training=True) # G_BA(A) ideally should be A\n",
        "\n",
        "            # Discriminator outputs for real images\n",
        "            disc_real_monet = self.m_disc(real_monet, training=True) # D_B(B_real)\n",
        "            disc_real_photo = self.p_disc(real_photo, training=True) # D_A(A_real)\n",
        "\n",
        "            # Discriminator outputs for fake images\n",
        "            disc_fake_monet = self.m_disc(fake_monet, training=True) # D_B(B_fake)\n",
        "            disc_fake_photo = self.p_disc(fake_photo, training=True) # D_A(A_fake)\n",
        "\n",
        "            # Generator adversarial losses\n",
        "            monet_gen_adv_loss = self.gen_loss_fn(disc_fake_monet) # For G_AB\n",
        "            photo_gen_adv_loss = self.gen_loss_fn(disc_fake_photo) # For G_BA\n",
        "\n",
        "            # Cycle consistency losses (L1 norm)\n",
        "            forward_cycle_loss = self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n",
        "            backward_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle)\n",
        "            total_cycle_loss = forward_cycle_loss + backward_cycle_loss\n",
        "\n",
        "            # Identity losses (L1 norm)\n",
        "            monet_identity_loss = self.identity_loss_fn(real_monet, same_monet, current_lambda_identity)\n",
        "            photo_identity_loss = self.identity_loss_fn(real_photo, same_photo, current_lambda_identity)\n",
        "\n",
        "            # Total generator losses\n",
        "            total_monet_gen_loss = monet_gen_adv_loss + total_cycle_loss + monet_identity_loss\n",
        "            total_photo_gen_loss = photo_gen_adv_loss + total_cycle_loss + photo_identity_loss\n",
        "\n",
        "            # Discriminator losses\n",
        "            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n",
        "            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n",
        "\n",
        "        # Calculate gradients\n",
        "        m_gen_grads = tape.gradient(total_monet_gen_loss, self.m_gen.trainable_variables)\n",
        "        p_gen_grads = tape.gradient(total_photo_gen_loss, self.p_gen.trainable_variables)\n",
        "        m_disc_grads = tape.gradient(monet_disc_loss, self.m_disc.trainable_variables)\n",
        "        p_disc_grads = tape.gradient(photo_disc_loss, self.p_disc.trainable_variables)\n",
        "\n",
        "        # Apply gradients\n",
        "        self.m_gen_optimizer.apply_gradients(zip(m_gen_grads, self.m_gen.trainable_variables))\n",
        "        self.p_gen_optimizer.apply_gradients(zip(p_gen_grads, self.p_gen.trainable_variables))\n",
        "        self.m_disc_optimizer.apply_gradients(zip(m_disc_grads, self.m_disc.trainable_variables))\n",
        "        self.p_disc_optimizer.apply_gradients(zip(p_disc_grads, self.p_disc.trainable_variables))\n",
        "\n",
        "        return {\n",
        "            \"monet_gen_loss\": total_monet_gen_loss,\n",
        "            \"photo_gen_loss\": total_photo_gen_loss,\n",
        "            \"monet_disc_loss\": monet_disc_loss,\n",
        "            \"photo_disc_loss\": photo_disc_loss,\n",
        "            \"total_cycle_loss\": total_cycle_loss,\n",
        "            \"monet_identity_loss\": monet_identity_loss,\n",
        "            \"photo_identity_loss\": photo_identity_loss\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYhKtCDinlcR"
      },
      "source": [
        "## Defining Loss Functions\n",
        "The CycleGAN uses several types of loss functions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM4KkbPknrLL"
      },
      "source": [
        "**1. Adversarial Loss:** measures how well the generator can fool the discriminator.\n",
        "\n",
        "Typically Binary Cross-Entropy (BCE) loss.\n",
        "\n",
        "* For the discriminator, it's the sum of BCE for real images (target 1s) and fake images (target 0s).\n",
        "\n",
        "* For the generator, it's BCE for fake images (target 1s, as it tries to make them look real)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6-2QYumn6wY"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    # Using from_logits=True as discriminator output is a logit\n",
        "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "    def discriminator_loss(real, generated):\n",
        "        real_loss = bce(tf.ones_like(real), real)\n",
        "        generated_loss = bce(tf.zeros_like(generated), generated)\n",
        "        total_disc_loss = real_loss + generated_loss\n",
        "        # Average over the patch/image outputs and then over the batch\n",
        "        # For PatchGAN, the loss is averaged over all patches.\n",
        "        return tf.reduce_mean(total_disc_loss) * 0.5\n",
        "\n",
        "    def generator_loss(generated):\n",
        "        # Generator tries to make discriminator output 1 for fake images\n",
        "        return tf.reduce_mean(bce(tf.ones_like(generated), generated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX_77n7QoA4v"
      },
      "source": [
        "**2. Cycle Consistency Loss:** penalizes the difference (L1 norm) between an original image and its cycled version. This is crucial for unpaired translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdXOAwTOoIRs"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    def calc_cycle_loss(real_image, cycled_image, LAMBDA_CYCLE):\n",
        "        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
        "        return LAMBDA_CYCLE * loss1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjuyUmKYoLJQ"
      },
      "source": [
        "**3. Identity Loss:** encourages the generator to act as an identity mapping when given an image from its target domain. E.g., the photo-to-Monet generator should not significantly alter an image that is already a Monet painting. This helps preserve color composition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mU1YHWooS4h"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    def identity_loss(real_image, same_image, LAMBDA_IDENTITY):\n",
        "        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
        "        return LAMBDA_IDENTITY * loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLEqMfLooiGx"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyLN-H5IothY"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    # Optimizers\n",
        "    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "    # Create CycleGAN model instance\n",
        "    # lambda_identity is a factor of lambda_cycle (e.g., 0.5 * lambda_cycle)\n",
        "    cycle_gan_model = CycleGan(\n",
        "        monet_generator, photo_generator, monet_discriminator, photo_discriminator,\n",
        "        lambda_cycle=10, lambda_identity=0.5\n",
        "    )\n",
        "\n",
        "    # Compile the model\n",
        "    cycle_gan_model.compile(\n",
        "        m_gen_optimizer = monet_generator_optimizer,\n",
        "        p_gen_optimizer = photo_generator_optimizer,\n",
        "        m_disc_optimizer = monet_discriminator_optimizer,\n",
        "        p_disc_optimizer = photo_discriminator_optimizer,\n",
        "        gen_loss_fn = generator_loss,\n",
        "        disc_loss_fn = discriminator_loss,\n",
        "        cycle_loss_fn = calc_cycle_loss,\n",
        "        identity_loss_fn = identity_loss\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS_TO_TRAIN = 50  # Set to a higher number for actual training (e.g., 25)\n",
        "\n",
        "#Check if datasets are not empty\n",
        "if tf.data.experimental.cardinality(monet_ds).numpy() == 0 or tf.data.experimental.cardinality(photo_ds).numpy() == 0:\n",
        "    print(\"Error: One or both datasets are empty. Please check data loading.\")\n",
        "else:\n",
        "    print(f\"Starting training for {EPOCHS_TO_TRAIN} epochs...\")\n",
        "\n",
        "    # Uncomment the following block for actual training\n",
        "    history = cycle_gan_model.fit(\n",
        "        tf.data.Dataset.zip((monet_ds.repeat(), photo_ds.repeat())),\n",
        "        epochs=EPOCHS_TO_TRAIN,\n",
        "        steps_per_epoch=max(len(MONET_FILENAMES), len(PHOTO_FILENAMES))\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPvOSpPxo2h7"
      },
      "source": [
        "The cell below starts the training process. Training GANs can be time-consuming, especially without a GPU/TPU. For a demonstration, a few epochs might suffice to see some initial results. For good quality, more epochs (e.g., 25-100+) are usually needed.\n",
        "\n",
        "**Note:** We limited compute, consider training for a very small number of epochs (e.g., `epochs=1` or `epochs=5`) or skipping this step and directly loading pre-trained weights in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save trained models\n",
        "monet_generator.save('monet_generator_trained_model')  # Saves in SavedModel format\n",
        "photo_generator.save('photo_generator_trained_model')\n",
        "# Alternatively save just weights:\n",
        "# monet_generator.save_weights('monet_generator_trained.weights.h5')\n",
        "# photo_generator.save_weights('photo_generator_trained.weights.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUMgcRdwpezd"
      },
      "source": [
        "### Loading Pre-trained Weights\n",
        "For convenience, pre-trained weights for the Monet generator (trained for approximately 50 epochs on the Kaggle dataset) can be downloaded and loaded. This allows for immediate visualization without lengthy training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF0C6E3pMM1G",
        "outputId": "a4860d72-a8f9-467b-8683-4ff2a594e88b"
      },
      "outputs": [],
      "source": [
        "PRETRAINED_MODEL_PATH = \"models/monet_generator_50_epochs\"\n",
        "monet_generator_pretrained = None\n",
        "\n",
        "if Path(PRETRAINED_MODEL_PATH).exists():\n",
        "    try:\n",
        "        with strategy.scope():  # Use strategy scope if needed\n",
        "            monet_generator_pretrained = TFSMLayer(\n",
        "                PRETRAINED_MODEL_PATH,\n",
        "                call_endpoint=\"serving_default\"  # You might need to adjust this\n",
        "            )\n",
        "        print(\"Pre-trained Monet generator loaded successfully as TFSMLayer.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading pre-trained model: {e}\")\n",
        "else:\n",
        "    print(f\"Pre-trained model path {PRETRAINED_MODEL_PATH} not found. Skipping loading.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0W_4bNopyHb",
        "outputId": "cf8c0764-ae31-4755-d8b8-cd15fdbcbc9d"
      },
      "outputs": [],
      "source": [
        "PRETRAINED_MODEL_PATH = \"monet_generator_50_epochs\"\n",
        "monet_generator_pretrained = None\n",
        "\n",
        "if Path(PRETRAINED_MODEL_PATH).exists():\n",
        "    try:\n",
        "        with strategy.scope():\n",
        "            # Load as a inference-only layer\n",
        "            monet_generator_pretrained = TFSMLayer(\n",
        "                PRETRAINED_MODEL_PATH,\n",
        "                call_endpoint='serving_default'  # Adjust if your model uses a different endpoint\n",
        "            )\n",
        "        print(\"Pre-trained Monet generator loaded as TFSMLayer.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading pre-trained model: {e}\")\n",
        "else:\n",
        "    print(f\"Pre-trained model path {PRETRAINED_MODEL_PATH} not found. Skipping loading.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBtg9WF7pIL6"
      },
      "source": [
        "## Visualize Results\n",
        "After training (or by loading pre-trained weights), we can visualize the style transfer by taking a few sample photos and transforming them using the loaded (or trained) Monet generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcqZEV8oqj24"
      },
      "outputs": [],
      "source": [
        "def display_generated_images(generator_model, num_images=5):\n",
        "    if generator_model is None:\n",
        "        print(\"Generator model not available for visualization.\")\n",
        "        return\n",
        "    if not PHOTO_FILENAMES:\n",
        "        print(\"Photo filenames not loaded. Cannot generate images.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, num_images * 4))\n",
        "    for i in range(num_images):\n",
        "        rand_idx = random.randint(0, len(PHOTO_FILENAMES) - 1)\n",
        "        img_path = PHOTO_FILENAMES[rand_idx]\n",
        "        input_image = decode_image(img_path)\n",
        "        input_image_batch = tf.expand_dims(input_image, axis=0)  # Keep batch dim for model input\n",
        "\n",
        "        # Get prediction (handles both TFSMLayer and regular models)\n",
        "        prediction_output = generator_model(input_image_batch, training=False)\n",
        "\n",
        "        # Extract output based on model type\n",
        "        if isinstance(prediction_output, dict):  # TFSMLayer returns a dict\n",
        "            prediction = prediction_output.get('output_0', prediction_output.get('serving_default', None))\n",
        "            if prediction is None:\n",
        "                prediction = list(prediction_output.values())[0]\n",
        "        else:  # Regular model\n",
        "            prediction = prediction_output[0] if isinstance(prediction_output, (list, tuple)) else prediction_output\n",
        "\n",
        "        # Convert to numpy and remove batch dimension (if present)\n",
        "        prediction = np.squeeze(prediction.numpy())  # Removes extra dims (e.g., (1, 256, 256, 3) → (256, 256, 3))\n",
        "        input_image = input_image.numpy()  # Already (256, 256, 3)\n",
        "\n",
        "        # Denormalize (if normalized to [-1, 1])\n",
        "        display_input = (input_image * 0.5 + 0.5).clip(0, 1)  # clip to avoid overflow\n",
        "        display_prediction = (prediction * 0.5 + 0.5).clip(0, 1)\n",
        "\n",
        "        # Plot\n",
        "        plt.subplot(num_images, 2, i * 2 + 1)\n",
        "        plt.imshow(display_input)\n",
        "        plt.title(f\"Input Photo {i+1}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(num_images, 2, i * 2 + 2)\n",
        "        plt.imshow(display_prediction)\n",
        "        plt.title(f\"Monet-esque Output {i+1}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xweuyxyrqp0A",
        "outputId": "76beb7ec-ceff-411c-8513-5504f8838393"
      },
      "outputs": [],
      "source": [
        "# Visualize using the pre-trained model\n",
        "if monet_generator_pretrained:\n",
        "    display_generated_images(monet_generator_pretrained, num_images=5)\n",
        "else:\n",
        "    print(\"Skipping visualization as pre-trained model was not loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O43rEfR0qwF1"
      },
      "outputs": [],
      "source": [
        "# Visualize using the trained and saved model\n",
        "my_trained_generator = tf.keras.models.load_model('monet_generator_trained_model')\n",
        "display_generated_images(my_trained_generator, num_images=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzUScYW8rOBR"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated the implementation of a CycleGAN for artistic style transfer, specifically transforming photographs into Monet-like paintings. Key takeaways include understanding the CycleGAN architecture, the importance of cycle-consistency and identity losses for unpaired image translation, and the process of building, training, and evaluating such models.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
